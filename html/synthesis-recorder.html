<!DOCTYPE html>
<html>

<head>
  <link rel="stylesheet" href="style.css">
  <title>SpeechSynthesisRecorder.js - Record window.speechSynthesis.speak() with navigator.getUserMedia() and MediaRecorder</title>
</head>

<body>
  <script>
// SpeechSynthesisRecorder.js guest271314 6-17-2017
// Motivation: Get audio output from `window.speechSynthesis.speak()` call
// as `ArrayBuffer`, `AudioBuffer`, `Blob`, `MediaSource`, `MediaStream`, `ReadableStream`, or other object or data types
// See https://lists.w3.org/Archives/Public/public-speech-api/2017Jun/0000.html
// https://github.com/guest271314/SpeechSynthesisRecorder

// Configuration: Analog Stereo Duplex
// Input Devices: Monitor of Built-in Audio Analog Stereo, Built-in Audio Analog Stereo

/* global MediaRecorder SpeechSynthesisUtterance MediaStream MediaSource AudioContext navigator Blob ReadableStream URL Audio FileReader */
class SpeechSynthesisRecorder {
  constructor ({text = '', utteranceOptions = {}, recorderOptions = {}, dataType = ''}) {
    if (text === '') throw new Error('no words to synthesize')
    this.dataType = dataType
    this.text = text
    this.mimeType = MediaRecorder.isTypeSupported('audio/webm; codecs=opus')
                    ? 'audio/webm; codecs=opus' : 'audio/ogg; codecs=opus'
    this.utterance = new SpeechSynthesisUtterance(this.text)
    this.speechSynthesis = window.speechSynthesis
    this.mediaStream_ = new MediaStream()
    this.mediaSource_ = new MediaSource()
    this.mediaRecorder = new MediaRecorder(this.mediaStream_, {
      mimeType: this.mimeType,
      bitsPerSecond: 256 * 8 * 1024
    })
    this.audioContext = new AudioContext()
    this.audioNode = new Audio()
    this.chunks = []
    if (utteranceOptions) {
      if (utteranceOptions.voice) {
        this.speechSynthesis.onvoiceschanged = e => {
          const voice = this.speechSynthesis.getVoices().find(({
            name: _name
          }) => _name === utteranceOptions.voice)
          this.utterance.voice = voice
          console.log(voice, this.utterance)
        }
        this.speechSynthesis.getVoices()
      }
      let {
        lang, rate, pitch
      } = utteranceOptions
      Object.assign(this.utterance, {
        lang, rate, pitch
      })
    }
    this.audioNode.controls = 'controls'
    document.body.appendChild(this.audioNode)
  }
  start (text = '') {
    if (text) this.text = text
    if (this.text === '') throw new Error('no words to synthesize')
    return navigator.mediaDevices.getUserMedia({
      audio: true
    })
    // set `getUserMedia()` constraints to "auidooutput", where avaialable
    // see https://bugzilla.mozilla.org/show_bug.cgi?id=934425, https://stackoverflow.com/q/33761770
    .then(stream => navigator.mediaDevices.enumerateDevices()
      .then(devices => {
        const audiooutput = devices.find(device => device.kind == "audiooutput");
        stream.getTracks().forEach(track => track.stop())
        if (audiooutput) {
          const constraints = {
                  deviceId: {
                    exact: audiooutput.deviceId
                  }
                };
          return navigator.mediaDevices.getUserMedia({
            audio: constraints     
          });
        }
        return navigator.mediaDevices.getUserMedia({
          audio: true
        });
      }))
      .then(stream => new Promise(resolve => {
        const track = stream.getAudioTracks()[0]
        this.mediaStream_.addTrack(track)
        // return the current `MediaStream`
        if (this.dataType && this.dataType === 'mediaStream') {
          resolve({tts: this, data: this.mediaStream_})
        };
        this.mediaRecorder.ondataavailable = event => {
          if (event.data.size > 0) {
            this.chunks.push(event.data)
          };
        }
        this.mediaRecorder.onstop = () => {
          track.stop()
          this.mediaStream_.getAudioTracks()[0].stop()
          this.mediaStream_.removeTrack(track)
          console.log(`Completed recording ${this.utterance.text}`, this.chunks)
          resolve(this)
        }
        this.mediaRecorder.start()
        this.utterance.onstart = () => {
          console.log(`Starting recording SpeechSynthesisUtterance ${this.utterance.text}`)
        }
        this.utterance.onend = () => {
          this.mediaRecorder.stop()
          console.log(`Ending recording SpeechSynthesisUtterance ${this.utterance.text}`)
        }
        this.speechSynthesis.speak(this.utterance)
      }))
  }
  blob () {
    if (!this.chunks.length) throw new Error('no data to return')
    return Promise.resolve({
      tts: this,
      data: this.chunks.length === 1 ? this.chunks[0] : new Blob(this.chunks, {
        type: this.mimeType
      })
    })
  }
  arrayBuffer (blob) {
    if (!this.chunks.length) throw new Error('no data to return')
    return new Promise(resolve => {
      const reader = new FileReader()
      reader.onload = e => resolve(({
        tts: this,
        data: reader.result
      }))
      reader.readAsArrayBuffer(blob ? new Blob(blob, {
        type: blob.type
      }) : this.chunks.length === 1 ? this.chunks[0] : new Blob(this.chunks, {
        type: this.mimeType
      }))
    })
  }
  audioBuffer () {
    if (!this.chunks.length) throw new Error('no data to return')
    return this.arrayBuffer()
      .then(ab => this.audioContext.decodeAudioData(ab))
      .then(buffer => ({
        tts: this,
        data: buffer
      }))
  }
  mediaSource () {
    if (!this.chunks.length) throw new Error('no data to return')
    return this.arrayBuffer()
      .then(({
        data: ab
      }) => new Promise((resolve, reject) => {
        this.mediaSource_.onsourceended = () => resolve({
          tts: this,
          data: this.mediaSource_
        })
        this.mediaSource_.onsourceopen = () => {
          if (MediaSource.isTypeSupported(this.mimeType)) {
            const sourceBuffer = this.mediaSource_.addSourceBuffer(this.mimeType)
            sourceBuffer.mode = 'sequence'
            sourceBuffer.onupdateend = () =>
              this.mediaSource_.endOfStream()
            sourceBuffer.appendBuffer(ab)
          } else {
            reject(new Error(`${this.mimeType} is not supported`))
          }
        }
        this.audioNode.src = URL.createObjectURL(this.mediaSource_)
      }))
  }
  readableStream ({size = 1024, controllerOptions = {}, rsOptions = {}}) {
    if (!this.chunks.length) throw new Error('no data to return')
    const src = this.chunks.slice(0)
    const chunk = size
    return Promise.resolve({
      tts: this,
      data: new ReadableStream(controllerOptions || {
        start (controller) {
          console.log(src.length)
          controller.enqueue(src.splice(0, chunk))
        },
        pull (controller) {
          if (src.length === 0) controller.close()
          controller.enqueue(src.splice(0, chunk))
        }
      }, rsOptions)
    })
  }
}
if (typeof module !== 'undefined') module.exports = SpeechSynthesisRecorder
if (typeof window !== 'undefined') window.SpeechSynthesisRecorder = SpeechSynthesisRecorder


    let ttsRecorder = new SpeechSynthesisRecorder({text:"The revolution will not be televised", utteranceOptions:{
      voice: "english-us espeak",
      lang: "en-US",
      pitch: .75,
      rate: 1
    }});

    ttsRecorder.start()
      .then(tts => tts.blob())
      .then(({
        tts, data
      }) => {
        // do stuff with `ArrayBuffer`, `AudioBuffer`, `Blob`, `MediaSource`, `MediaStream`, `ReadableStream`
        console.log(tts, data);
        tts.audioNode.src = URL.createObjectURL(data);
        tts.audioNode.title = tts.utterance.text;
        tts.audioNode.onloadedmetadata = () => {
          console.log(tts.audioNode.duration);
          // tts.audioNode.play();
        }
      })
      .catch(err => console.log(err))
  </script>
</body>

</html>